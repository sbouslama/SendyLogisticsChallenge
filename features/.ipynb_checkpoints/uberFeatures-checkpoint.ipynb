{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Features\n",
    "Uber launched its Uber Movement service at the beginning of 2017. It consists of billions of pieces of trip data and provides access to the summary of travel times between different regions of the selected city. In this notebook, we will see how to extract maybe some useful features related to the sendy challenge such as the traffic congestions trends on a particular day of the week.\n",
    "\n",
    "You can download these datasets in a csv format from this [link](https://movement.uber.com/cities/nairobi/downloads/speeds?lang=en-US&tp[y]=2019&tp[q]=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding\n",
    "\n",
    "We’ll first go to the Uber Movement website and navigate our way to Nairobi. Then we’ll download the CSV file for “Weekly Aggregate” and \"monthly aggregate\" and \"hourly Aggregate\" since we dont have the year. In this case, I choosed the first Quarter since I have assumed that sendy provided us with her recent datasets.\n",
    "\n",
    "\n",
    "We’ll also need the geographical boundaries file to set regional coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed librairies\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from LIB.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real all datasets\n",
    "data_path = '../data/'\n",
    "hourly = pd.read_csv(data_path+'nairobi-hexclusters-2019-1-All-HourlyAggregate.csv')\n",
    "monthly = pd.read_csv(data_path+ 'nairobi-hexclusters-2019-2-All-MonthlyAggregate.csv')\n",
    "weekly = pd.read_csv(data_path+ 'nairobi-hexclusters-2019-2-WeeklyAggregate.csv')\n",
    "\n",
    "\n",
    "version = 'V0'\n",
    "train = pd.read_csv(data_path+'processed_data/train{}.csv'.format(version))\n",
    "test = pd.read_csv(data_path+'processed_data/test{}.csv'.format(version))\n",
    "# merge train and test so we can apply the same steps on it\n",
    "train['train'] = 1\n",
    "test['train'] = 0 \n",
    "all_data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the destination and pickup coordinates to tuple object\n",
    "all_data['coordinatesDestination'] = all_data[['Destination Long','Destination Lat']].apply(lambda row:tuple([row[0],row[1]]), axis=1)\n",
    "all_data['coordinatesPickup'] = all_data[['Pickup Long','Pickup Lat']].apply(lambda row:tuple([row[0],row[1]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfor the json geographical boundaries file to a dataFrame\n",
    "#read geojson file \n",
    "movement_id, geometry = [],[]\n",
    "with open(data_path+'nairobi_hexclusters.json') as json_file:\n",
    "    data = json.load(json_file)  \n",
    "    for result in data['features']:\n",
    "        movement_id.append(result[u'properties'][u'MOVEMENT_ID'])\n",
    "        geometry.append(result[u'geometry'][u'coordinates'][0])\n",
    "    df = pd.DataFrame([movement_id,geometry]).T\n",
    "df.columns = ['movement_id','geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate to each coordinates the corresponding hexapolygone cluster\n",
    "polygones = df['geometry']\n",
    "def which_polygone(values, name_col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "     values: raw values for the each sample\n",
    "     name_col: the name of the column to process  \n",
    "    \"\"\"\n",
    "    coordinates = values['coordinates'+name_col]\n",
    "    for index, polygone in enumerate(polygones):\n",
    "        if Point(coordinates).within(Polygon(polygone)):\n",
    "            return pd.Series([df.iloc[index,0]], index=[name_col+'Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicate the function wich polygone to both the destination and pickp coordinates \n",
    "tqdm.pandas()\n",
    "all_data['DestinationId']= all_data.progress_apply(which_polygone, args=('Destination',),axis=1)\n",
    "all_data['PickupId'] = all_data.progress_apply(which_polygone, args=('Pickup',),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing some of the columns's name to make the join easier \n",
    "all_data.rename(columns= {\n",
    "    \"DestinationId\":\"dstid\",\n",
    "    \"PickupId\":\"sourceid\",\n",
    "    \"Placement - Weekday (Mo = 1)\":\"dow\",\n",
    "    \"Pickup - TimeHour\":\"hod\"\n",
    "}, inplace=True)\n",
    "\n",
    "# transform the columns to join on to the same dtype\n",
    "weekly.sourceid = weekly.sourceid.astype(str)\n",
    "weekly.dstid = weekly.dstid.astype(str)\n",
    "\n",
    "hourly.sourceid = hourly.sourceid.astype(str)\n",
    "hourly.dstid = hourly.dstid.astype(str)\n",
    "\n",
    "all_data.sourceid = all_data.sourceid.astype(str)\n",
    "all_data.dstid = all_data.dstid.astype(str)\n",
    "\n",
    "# merge the weekly and hourly datasets\n",
    "all_data = all_data.merge(weekly, on=['sourceid','dstid','dow'], how='left')\n",
    "all_data = all_data.merge(hourly, on=['sourceid','dstid','hod'], how='left', suffixes=('_hourly','_hourly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(all_data.coordinatesDestination, all_data.coordinatesDestination)\n",
    "train, test = all_data[all_data.train==1], all_data[all_data.train==0]\n",
    "del(train.train, test.train)\n",
    "version = 'V3'\n",
    "train.to_csv(DATA_PATH+'processed_data/train{}.csv'.format(version), index=False)\n",
    "test.to_csv(DATA_PATH+'processed_data/test{].csv'.format(version), index=False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
